#!/usr/bin/env python
"""
Script to create a small sample dataset for testing RAG-CSD.
"""

import argparse
import logging
import os
import random
import sys
from typing import Dict, List

# Add the parent directory to sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from rag_csd.utils.logger import setup_logger


# Sample topics and information about computational storage
TOPICS = {
    "computational_storage": [
        """
        Computational Storage is a new computing paradigm that moves processing closer to where data is stored.
        It helps reduce data movement and improves performance for data-intensive applications.
        Computational Storage Devices (CSDs) integrate computing capabilities directly with storage, enabling
        operations like filtering, aggregation, and transformation to be performed within the storage device itself.
        """,
        """
        Key benefits of Computational Storage include reduced latency, decreased power consumption, improved
        application performance, and better resource utilization. By minimizing data movement between storage
        and the CPU, computational storage reduces the bottlenecks in traditional computing architectures.
        This approach is particularly valuable for applications dealing with large datasets, such as databases,
        analytics, and machine learning.
        """,
        """
        The architecture of a typical CSD includes storage media (like NAND flash), an embedded processor
        (ARM cores, FPGA, or ASIC), and firmware that exposes computational capabilities. CSDs can be
        implemented as SSDs with added compute, FPGAs with storage, or custom ASICs. These devices typically
        connect via NVMe or PCIe interfaces and may support standards like NVMe Computational Storage.
        """
    ],
    "vector_databases": [
        """
        Vector databases are specialized database systems designed to store, manage, and search vector
        embeddings efficiently. These embeddings are numerical representations of data objects such as
        text, images, or audio, generated by machine learning models. Vector databases provide specialized
        indexes and algorithms for similarity search operations, which are essential for applications like
        semantic search, recommendation systems, and anomaly detection.
        """,
        """
        The core functionality of vector databases is approximate nearest neighbor (ANN) search, which
        finds the most similar vectors to a query vector based on distance metrics like cosine similarity,
        Euclidean distance, or dot product. Unlike traditional databases that excel at exact matches,
        vector databases optimize for similarity-based retrieval, trading off some accuracy for significant
        performance improvements.
        """,
        """
        Common indexing techniques in vector databases include tree-based methods (KD-trees, VP-trees),
        graph-based approaches (HNSW, NSG), and quantization methods (PQ, OPQ). These approaches create
        data structures that enable efficient navigation of the vector space to find nearest neighbors
        without exhaustive comparison. Popular vector database systems include FAISS (Facebook AI Similarity Search),
        Milvus, Pinecone, Weaviate, and Qdrant.
        """
    ],
    "retrieval_augmented_generation": [
        """
        Retrieval-Augmented Generation (RAG) is a hybrid AI framework that combines the strengths of
        retrieval-based systems with generative models. In a RAG system, a retrieval component first
        fetches relevant information from a knowledge base, and then a generative model uses this
        retrieved context to produce more accurate, factual, and contextually appropriate responses.
        This approach enhances the capabilities of large language models by giving them access to
        external knowledge beyond their training data.
        """,
        """
        The typical RAG architecture consists of three main components: (1) an embedding model that
        converts queries and documents into vector representations, (2) a retrieval system that finds
        the most relevant documents using vector similarity, and (3) a generation model that synthesizes
        the original query with the retrieved information to produce a response. This pipeline allows
        the system to ground its generations in specific, relevant facts from a trustworthy knowledge base.
        """,
        """
        RAG offers several advantages over pure generative models, including improved factuality,
        reduced hallucination, greater transparency (as sources can be cited), and the ability to
        access up-to-date information without retraining. RAG systems are particularly valuable for
        question answering, chatbots, summarization, and any application where factual accuracy and
        recency of information are important. They can be customized by changing the knowledge base
        to adapt to different domains or use cases.
        """
    ],
    "storage_computing_integration": [
        """
        The integration of storage and computing represents a fundamental shift in system architecture,
        moving away from the traditional von Neumann model where data shuttles between separate storage
        and processing units. This integration takes various forms, including computational storage,
        near-data processing, in-memory computing, and processing-in-memory. These approaches share the
        common goal of reducing data movement, which has become a major bottleneck in terms of latency,
        energy consumption, and overall system performance.
        """,
        """
        Storage-computing integration offers significant benefits for data-intensive applications.
        By performing computations closer to where data resides, systems can achieve lower latency,
        higher throughput, and improved energy efficiency. This paradigm shift is particularly
        relevant for big data analytics, AI/ML workloads, database operations, and edge computing
        scenarios where processing large volumes of data with limited resources is essential.
        """,
        """
        Implementation approaches for storage-computing integration include specialized hardware
        (CSDs, smart SSDs, FPGAs with embedded storage), software frameworks that optimize data
        locality, and hybrid architectures that selectively offload operations. Challenges in this
        domain include programming model complexity, cost-effectiveness, standardization, and
        compatibility with existing software ecosystems. Despite these challenges, the trend toward
        tighter storage-computing integration continues to grow as data volumes increase and
        traditional architectures reach their scaling limits.
        """
    ],
    "vector_similarity_search": [
        """
        Vector similarity search is the computational process of finding vectors in a dataset that
        are most similar to a query vector according to some distance metric. This operation forms
        the foundation of many modern AI applications, including semantic search, recommendation
        systems, image retrieval, and anomaly detection. The goal is to efficiently identify the
        k-nearest neighbors (kNN) to the query vector in a potentially very large vector space.
        """,
        """
        Common distance metrics used in vector similarity search include cosine similarity (measuring
        the angle between vectors, ideal for text embeddings), Euclidean distance (measuring the
        straight-line distance, suitable for spatial data), dot product (measuring directional similarity),
        and Manhattan distance (measuring grid-like distance). The choice of metric depends on the nature
        of the data and the specific requirements of the application.
        """,
        """
        Scaling vector similarity search to large datasets presents significant challenges. Exact
        nearest neighbor search becomes prohibitively expensive as dimensions and dataset size increase,
        a phenomenon known as the "curse of dimensionality." To address this, approximate nearest
        neighbor (ANN) algorithms sacrifice some accuracy for dramatic performance improvements.
        Popular ANN techniques include locality-sensitive hashing (LSH), hierarchical navigable small
        world graphs (HNSW), product quantization (PQ), and inverted file with product quantization (IVF+PQ).
        """
    ]
}


def create_directory(path: str) -> None:
    """Create a directory if it doesn't exist."""
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)
        logging.info(f"Created directory: {path}")


def create_sample_data(output_dir: str, num_docs_per_topic: int = 3) -> List[str]:
    """
    Create sample data documents.
    
    Args:
        output_dir: Directory to save the sample data.
        num_docs_per_topic: Number of documents to create per topic.
        
    Returns:
        List of file paths created.
    """
    create_directory(output_dir)
    
    file_paths = []
    
    for topic, paragraphs in TOPICS.items():
        for i in range(num_docs_per_topic):
            # Create file name
            file_name = f"{topic}_{i+1}.txt"
            file_path = os.path.join(output_dir, file_name)
            
            # Select paragraphs for this document
            # We'll randomize paragraph selection to create variety
            selected_paragraphs = random.sample(paragraphs, k=min(2, len(paragraphs)))
            
            # Add a custom title
            title = f"Document about {topic.replace('_', ' ').title()}"
            content = f"{title}\n\n" + "\n\n".join(selected_paragraphs)
            
            # Write to file
            with open(file_path, "w") as f:
                f.write(content)
            
            file_paths.append(file_path)
            logging.info(f"Created sample document: {file_path}")
    
    return file_paths


def create_sample_queries(output_dir: str) -> List[Dict]:
    """
    Create sample queries for testing.
    
    Args:
        output_dir: Directory to save the sample queries.
        
    Returns:
        List of query dictionaries.
    """
    queries = [
        {
            "id": "q1",
            "text": "What is computational storage?",
            "topic": "computational_storage"
        },
        {
            "id": "q2",
            "text": "How do vector databases work?",
            "topic": "vector_databases"
        },
        {
            "id": "q3",
            "text": "Explain retrieval augmented generation.",
            "topic": "retrieval_augmented_generation"
        },
        {
            "id": "q4",
            "text": "What are the benefits of integrating storage and computing?",
            "topic": "storage_computing_integration"
        },
        {
            "id": "q5",
            "text": "How does vector similarity search work?",
            "topic": "vector_similarity_search"
        },
        {
            "id": "q6",
            "text": "What are the advantages of computational storage devices?",
            "topic": "computational_storage"
        },
        {
            "id": "q7",
            "text": "Compare different vector database indexing techniques.",
            "topic": "vector_databases"
        },
        {
            "id": "q8",
            "text": "What is the architecture of a RAG system?",
            "topic": "retrieval_augmented_generation"
        },
        {
            "id": "q9",
            "text": "What challenges exist in storage-computing integration?",
            "topic": "storage_computing_integration"
        },
        {
            "id": "q10",
            "text": "Explain approximate nearest neighbor search algorithms.",
            "topic": "vector_similarity_search"
        }
    ]
    
    # Save queries to file
    file_path = os.path.join(output_dir, "test_queries.json")
    
    import json
    with open(file_path, "w") as f:
        json.dump(queries, f, indent=2)
    
    logging.info(f"Created sample queries: {file_path}")
    
    return queries


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Create sample data for RAG-CSD testing.")
    parser.add_argument(
        "--output-dir", "-o", type=str, default="data/raw", help="Output directory for sample data."
    )
    parser.add_argument(
        "--query-dir", "-q", type=str, default="data", help="Output directory for sample queries."
    )
    parser.add_argument(
        "--num-docs", "-n", type=int, default=3, help="Number of documents per topic."
    )
    parser.add_argument("--log-level", "-l", type=str, default="INFO", help="Logging level.")
    
    args = parser.parse_args()
    
    # Setup logger
    setup_logger(level=args.log_level)
    
    # Create sample data
    logging.info(f"Creating sample data in {args.output_dir}")
    file_paths = create_sample_data(args.output_dir, args.num_docs)
    logging.info(f"Created {len(file_paths)} sample documents")
    
    # Create sample queries
    logging.info(f"Creating sample queries in {args.query_dir}")
    queries = create_sample_queries(args.query_dir)
    logging.info(f"Created {len(queries)} sample queries")
    
    logging.info("Sample data creation complete")


if __name__ == "__main__":
    main()